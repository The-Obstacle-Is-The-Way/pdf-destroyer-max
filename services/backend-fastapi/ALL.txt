Dockerfile

# Use NVIDIA CUDA base image with Ubuntu 20.04
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu20.04

# Set environment variables for CUDA and Python
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_HOME=/usr/local/cuda-11.8
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install Python 3.10 and dependencies
RUN apt-get update && \
    apt-get install -y software-properties-common && \
    add-apt-repository ppa:deadsnakes/ppa && \
    apt-get update && \
    apt-get install -y python3.10 python3.10-distutils python3.10-dev gcc g++ curl && \
    rm -rf /var/lib/apt/lists/*

# Install pip cleanly using get-pip.py
RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10

# Set the working directory
WORKDIR /app

# Copy requirements.txt first to leverage Docker cache
COPY requirements.txt /app/

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application files
COPY . /app/

# Expose the service port
EXPOSE 8001

# Define the command to run the application using uvicorn directly
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001", "--reload"]

---------------------------------------------

main.py
# services/backend-fastapi/main.py

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from api.routes import summarization_routes, pdf_routes, health
import uvicorn
from config.settings import Settings
from utils.logging import logger, setup_logging

# Setup logging first
setup_logging()

# Load settings
settings = Settings()

app = FastAPI(
    title="PDF Destroyer Max API",
    description="Backend service for PDF processing and analysis",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(summarization_routes.router)
app.include_router(pdf_routes.router)
app.include_router(health.router)

@app.get("/")
async def root():
    logger.info("Root endpoint accessed")
    return {
        "message": "PDF Destroyer Max API is running",
        "version": "1.0.0",
        "docs_url": "/docs"
    }

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.exception(f"Global exception handler caught: {str(exc)}")
    return {"detail": "Internal server error"}, 500

if __name__ == "__main__":
    try:
        logger.info(f"Starting server on {settings.host}:{settings.port}")
        uvicorn.run(
            "main:app",
            host=settings.host,
            port=settings.port,
            reload=settings.debug_mode,
            workers=settings.worker_count
        )
    except Exception as e:
        logger.exception(f"Failed to start server: {str(e)}")
        raise


---------------------------------------------

# FastAPI and Core Dependencies
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.4.2
pydantic-settings==2.0.3
python-multipart==0.0.6
# Logging
loguru==0.7.2
# HTTP Client
httpx==0.25.1
# Async Support
aiofiles==23.2.1
tenacity==8.2.3
# Environment and Config
python-dotenv==1.0.0
# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0
requests==2.31.0
# Image Processing
Pillow==10.1.0
# PDF Processing
PyPDF2==3.0.1
pdf2image==1.16.3
# Utilities
python-jose[cryptography]==3.3.0  # For JWT
passlib[bcrypt]==1.7.4  # For password hashing

------------------------------------


"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\dependencies.py"

from functools import lru_cache
from config.settings import Settings, get_settings
from .summarization_service import SummarizationService
from .ocr_service import OCRService
from .pdf_pipeline import PDFPipelineService

@lru_cache()
def get_summarization_service(settings: Settings = get_settings()) -> SummarizationService:
    """Dependency provider for SummarizationService"""
    return SummarizationService(settings)

@lru_cache()
def get_ocr_service(settings: Settings = get_settings()) -> OCRService:
    """Dependency provider for OCRService"""
    return OCRService(settings)

@lru_cache()
def get_pdf_pipeline_service(settings: Settings = get_settings()) -> PDFPipelineService:
    """Dependency provider for PDFPipelineService"""
    return PDFPipelineService(settings)


------------------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\ocr_service.py"
# backend-fastapi/api/ocr_service.py
from typing import Dict, Any, Optional, BinaryIO
from fastapi import HTTPException, BackgroundTasks, UploadFile
import httpx
from datetime import datetime
import logging
from PIL import Image
import io

logger = logging.getLogger(__name__)

class OCRService:
    def __init__(self, settings):
        self.base_url = settings.ocr_service_url or "http://neural-ocr-tesseract:8002"
        self.timeout = httpx.Timeout(30.0)

    async def _get_client(self) -> httpx.AsyncClient:
        return httpx.AsyncClient(
            base_url=self.base_url,
            timeout=self.timeout
        )

    async def extract_text(self, file: UploadFile) -> Dict[str, Any]:
        try:
            async with await self._get_client() as client:
                files = {'file': (file.filename, await file.read(), file.content_type)}
                response = await client.post("/ocr", files=files)
                response.raise_for_status()
                return response.json()
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 400:
                raise HTTPException(400, "Invalid image format")
            logger.error(f"OCR service error: {str(e)}")
            raise HTTPException(503, "OCR service unavailable")
        except Exception as e:
            logger.error(f"OCR processing error: {str(e)}")
            raise HTTPException(500, str(e))

    async def health_check(self) -> Dict[str, Any]:
        try:
            async with await self._get_client() as client:
                start = datetime.utcnow()
                response = await client.get("/health")
                latency = (datetime.utcnow() - start).total_seconds() * 1000

                if response.status_code == 200:
                    data = response.json()
                    return {
                        'healthy': True,
                        'latency_ms': latency,
                        'gpu': data.get('gpu'),
                        'service': data.get('service')
                    }
                return {'healthy': False}
        except Exception as e:
            logger.error(f"Health check failed: {str(e)}")
            return {'healthy': False}

------------------------------------


"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\pdf_pipeline.py"

# services/backend-fastapi/api/pdf_pipeline.py

from pathlib import Path
import asyncio
from loguru import logger
from datetime import datetime
from typing import Dict, List

# Import our parallel processing components
from ai_powerhouse_pipelines.src.orchestration.smart_orchestrator import SmartOrchestrator
from ai_powerhouse_pipelines.src.models.neural_merger.merger import NeuralMerger
from ai_powerhouse_pipelines.src.models.quality_scoring.scorer import QualityScorer
from neural_ocr_tesseract.src.parallel_processing.ocr_worker import OCRWorker
from pdf_processor.src.parallel_processors.text_extractor import ParallelTextExtractor

class ProcessingOrchestrator:
    """Enhanced orchestrator that uses parallel processing"""
    
    def __init__(self):
        # Initialize directories from settings
        self.upload_dir = service_settings.upload_dir
        self.results_dir = service_settings.processed_dir
        self.failed_dir = service_settings.failed_dir
        
        # Create parallel processing pipeline
        self.orchestrator = SmartOrchestrator()
        self.neural_merger = NeuralMerger()
        self.quality_scorer = QualityScorer()
        
        # Initialize processors
        self._setup_processors()
        logger.info("Initialized enhanced ProcessingOrchestrator with parallel processing")

    async def _setup_processors(self):
        """Initialize and register all processors"""
        # Create processor instances
        text_extractor = ParallelTextExtractor(
            min_text_length=service_settings.min_text_length
        )
        
        ocr_worker = OCRWorker(
            language=service_settings.tesseract_language,
            dpi=service_settings.image_quality
        )
        
        # Register with orchestrator
        await self.orchestrator.register_processor(text_extractor)
        await self.orchestrator.register_processor(ocr_worker)
        
        # Set up neural merger in orchestrator
        self.orchestrator.neural_merger = self.neural_merger
        self.orchestrator.quality_scorer = self.quality_scorer

    async def process_pdf(self, task_id: str, file_path: Path):
        """Process PDF using parallel architecture"""
        try:
            processing_tasks[task_id].status = "PROCESSING"
            logger.info(f"Starting parallel processing for task {task_id}")
            
            # Run parallel processing
            results = await self.orchestrator.process_document(file_path)
            
            # Organize results
            all_chunks = []
            for page_num, content in results.items():
                # Score the quality
                quality_score = await self.quality_scorer.score_text(content)
                
                extraction_result[page_num] = {
                    'text': content,
                    'quality_score': quality_score,
                    'processing_method': 'parallel',
                }
            
            # Create processing result
            result = ProcessingResult(
                task_id=task_id,
                timestamp=datetime.utcnow(),
                page_count=len(results),
                chunk_count=len(all_chunks),
                content=extraction_result,
                metadata={
                    'processing_method': 'parallel',
                    'processors_used': list(self.orchestrator.processors.keys()),
                    'neural_merger_used': True
                }
            )
            
            # Save and update status
            await self.save_result(task_id, result)
            processing_tasks[task_id].status = "COMPLETED"
            processing_tasks[task_id].result = result
            
            logger.info(f"Completed parallel processing for task {task_id}")
            
        except Exception as e:
            logger.error(f"Error in parallel processing for task {task_id}: {str(e)}")
            processing_tasks[task_id].status = "FAILED"
            processing_tasks[task_id].error = str(e)
            # Move to failed directory
            failed_path = self.failed_dir / file_path.name
            file_path.rename(failed_path)

    async def save_result(self, task_id: str, result: ProcessingResult):
        """Saves processing results to disk"""
        result_path = self.results_dir / f"{task_id}.json"
        async with aiofiles.open(result_path, 'w') as f:
            await f.write(result.json())

------------------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\summarization_service.py"

from typing import List, Optional, Dict, Any
import httpx
import asyncio
from pydantic import BaseModel, Field, validator
from fastapi import HTTPException, BackgroundTasks
import logging
from utils.helpers import async_retry  # Changed to absolute import
import os
from datetime import datetime
from config.settings import Settings

logger = logging.getLogger(__name__)

class SummarizationRequest(BaseModel):
    """Request model for summarization"""
    text: str = Field(..., description="Text to summarize")
    max_length: Optional[int] = Field(130, ge=1, le=1000, description="Maximum length of generated summary")
    min_length: Optional[int] = Field(30, ge=1, description="Minimum length of generated summary")
    do_sample: Optional[bool] = Field(False, description="Whether to use sampling in generation")
    model_name: Optional[str] = Field("facebook/bart-large-cnn", description="Model to use for summarization")
    num_beams: Optional[int] = Field(4, ge=1, le=8, description="Number of beams for beam search")
    length_penalty: Optional[float] = Field(2.0, ge=0.0, le=5.0, description="Length penalty")

    @validator('text')
    def text_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('Text cannot be empty')
        return v.strip()

class SummaryResponse(BaseModel):
    """Response model for summarization"""
    summary: str
    model_used: str
    processing_time: float
    text_length: int
    summary_length: int
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class SummarizationService:
    """Service for text summarization with chunking support"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.transformer_url = settings.transformer_service_url
        self.timeout = httpx.Timeout(settings.transformer_timeout)
        self.max_retries = settings.max_retries
        self.chunk_size = 1024
        self.max_parallel_requests = 3
        
    async def _get_client(self) -> httpx.AsyncClient:
        """Create and configure HTTP client with connection pooling"""
        return httpx.AsyncClient(
            base_url=self.transformer_url,
            timeout=self.timeout,
            limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
            http2=True
        )

    def _chunk_text(self, text: str, chunk_size: int = 1024) -> List[str]:
        """Split text into chunks of roughly equal size, preserving sentence boundaries"""
        # Simple sentence splitting - can be improved with nltk or spacy
        sentences = text.replace('! ', '!|').replace('? ', '?|').replace('. ', '.|').split('|')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence.split())
            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
                
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        return chunks

    @retry_async(max_retries=3, delay=1)
    async def _process_chunk(self, chunk: str, client: httpx.AsyncClient, params: Dict) -> str:
        """Process a single chunk of text with retries"""
        try:
            start_time = datetime.utcnow()
            response = await client.post(
                "/generate",
                json={
                    "inputs": chunk,
                    "parameters": {
                        "max_length": params.get('max_length', self.settings.max_length),
                        "min_length": params.get('min_length', self.settings.min_length),
                        "do_sample": params.get('do_sample', False),
                        "num_beams": params.get('num_beams', self.settings.num_beams),
                        "length_penalty": params.get('length_penalty', self.settings.length_penalty),
                        "model_name": params.get('model_name', self.settings.default_model)
                    }
                }
            )
            response.raise_for_status()
            logger.info(f"Chunk processed in {(datetime.utcnow() - start_time).total_seconds():.2f}s")
            return response.json()["summary"]
            
        except httpx.HTTPError as e:
            logger.error(f"HTTP error processing chunk: {str(e)}")
            raise HTTPException(status_code=503, detail="Summarization service unavailable")
            
        except Exception as e:
            logger.error(f"Error processing chunk: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    async def summarize_text(
        self, 
        request: SummarizationRequest,
        background_tasks: BackgroundTasks,
        max_length: Optional[int] = None
    ) -> SummaryResponse:
        """
        Summarize text with support for long documents through chunking
        
        Args:
            request: SummarizationRequest containing text and parameters
            background_tasks: FastAPI BackgroundTasks for cleanup
            max_length: Optional override for max_length parameter
            
        Returns:
            SummaryResponse with generated summary and metadata
        """
        start_time = datetime.utcnow()
        
        try:
            # Update request parameters with any overrides
            request_params = request.dict()
            if max_length:
                request_params['max_length'] = max_length

            async with await self._get_client() as client:
                # For short texts, process directly
                if len(request.text.split()) <= self.chunk_size:
                    summary = await self._process_chunk(request.text, client, request_params)
                
                # For long texts, process in chunks with controlled concurrency
                else:
                    chunks = self._chunk_text(request.text, self.chunk_size)
                    logger.info(f"Processing text in {len(chunks)} chunks")
                    
                    semaphore = asyncio.Semaphore(self.max_parallel_requests)
                    
                    async def process_with_semaphore(chunk: str) -> str:
                        async with semaphore:
                            return await self._process_chunk(chunk, client, request_params)
                    
                    chunk_summaries = await asyncio.gather(
                        *[process_with_semaphore(chunk) for chunk in chunks],
                        return_exceptions=True
                    )
                    
                    # Handle any failed chunks
                    failed_chunks = [i for i, x in enumerate(chunk_summaries) if isinstance(x, Exception)]
                    if failed_chunks:
                        logger.error(f"Failed to process chunks: {failed_chunks}")
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to process {len(failed_chunks)} chunks"
                        )
                    
                    # Combine chunk summaries and create final summary if needed
                    combined_summary = " ".join(chunk_summaries)
                    if len(combined_summary.split()) > request_params.get('max_length', self.settings.max_length):
                        summary = await self._process_chunk(combined_summary, client, request_params)
                    else:
                        summary = combined_summary

                processing_time = (datetime.utcnow() - start_time).total_seconds()
                
                # Create response with metadata
                response = SummaryResponse(
                    summary=summary,
                    model_used=request_params.get('model_name', self.settings.default_model),
                    processing_time=processing_time,
                    text_length=len(request.text.split()),
                    summary_length=len(summary.split()),
                    metadata={
                        "chunks_processed": len(chunks) if len(request.text.split()) > self.chunk_size else 1,
                        "parameters": request_params,
                        "transformer_service": self.transformer_url
                    }
                )
                
                # Add cleanup task
                background_tasks.add_task(self._cleanup_resources, request.text)
                
                return response

        except asyncio.TimeoutError:
            logger.error("Timeout while connecting to transformer service")
            raise HTTPException(status_code=504, detail="Summarization service timeout")
            
        except Exception as e:
            logger.error(f"Error in summarization service: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=str(e))

    async def batch_summarize(
        self,
        requests: List[SummarizationRequest],
        background_tasks: BackgroundTasks
    ) -> List[SummaryResponse]:
        """Process multiple summarization requests in parallel"""
        tasks = [
            self.summarize_text(req, background_tasks)
            for req in requests
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)

    async def health_check(self) -> Dict:
        """Check if the transformer service is healthy"""
        try:
            async with await self._get_client() as client:
                start_time = datetime.utcnow()
                response = await client.get("/health")
                latency = (datetime.utcnow() - start_time).total_seconds() * 1000
                
                if response.status_code == 200:
                    return {
                        "healthy": True,
                        "details": {
                            "transformer_service": "healthy",
                            "latency_ms": latency,
                            "model_loaded": True
                        }
                    }
                return {
                    "healthy": False,
                    "details": {
                        "transformer_service": "unhealthy",
                        "status_code": response.status_code
                    }
                }
        except Exception as e:
            logger.error(f"Health check failed: {str(e)}")
            return {
                "healthy": False,
                "details": {
                    "transformer_service": "unhealthy",
                    "error": str(e)
                }
            }

    async def _cleanup_resources(self, text: str):
        """Cleanup any temporary resources"""
        try:
            # Implement resource cleanup as needed
            pass
        except Exception as e:
            logger.error(f"Cleanup failed: {str(e)}")

from typing import List, Optional, Dict, Any
import httpx
import asyncio
from pydantic import BaseModel, Field, validator
from fastapi import HTTPException, BackgroundTasks
import logging
from utils.helpers import async_retry  # Changed to absolute import
import os
from datetime import datetime
from config.settings import Settings

logger = logging.getLogger(__name__)

class SummarizationRequest(BaseModel):
    """Request model for summarization"""
    text: str = Field(..., description="Text to summarize")
    max_length: Optional[int] = Field(130, ge=1, le=1000, description="Maximum length of generated summary")
    min_length: Optional[int] = Field(30, ge=1, description="Minimum length of generated summary")
    do_sample: Optional[bool] = Field(False, description="Whether to use sampling in generation")
    model_name: Optional[str] = Field("facebook/bart-large-cnn", description="Model to use for summarization")
    num_beams: Optional[int] = Field(4, ge=1, le=8, description="Number of beams for beam search")
    length_penalty: Optional[float] = Field(2.0, ge=0.0, le=5.0, description="Length penalty")

    @validator('text')
    def text_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('Text cannot be empty')
        return v.strip()

class SummaryResponse(BaseModel):
    """Response model for summarization"""
    summary: str
    model_used: str
    processing_time: float
    text_length: int
    summary_length: int
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metadata: Dict[str, Any] = Field(default_factory=dict)

class SummarizationService:
    """Service for text summarization with chunking support"""
    
    def __init__(self, settings: Settings):
        self.settings = settings
        self.transformer_url = settings.transformer_service_url
        self.timeout = httpx.Timeout(settings.transformer_timeout)
        self.max_retries = settings.max_retries
        self.chunk_size = 1024
        self.max_parallel_requests = 3
        
    async def _get_client(self) -> httpx.AsyncClient:
        """Create and configure HTTP client with connection pooling"""
        return httpx.AsyncClient(
            base_url=self.transformer_url,
            timeout=self.timeout,
            limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
            http2=True
        )

    def _chunk_text(self, text: str, chunk_size: int = 1024) -> List[str]:
        """Split text into chunks of roughly equal size, preserving sentence boundaries"""
        # Simple sentence splitting - can be improved with nltk or spacy
        sentences = text.replace('! ', '!|').replace('? ', '?|').replace('. ', '.|').split('|')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence.split())
            if current_size + sentence_size > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
                
        if current_chunk:
            chunks.append(' '.join(current_chunk))
            
        return chunks

    @retry_async(max_retries=3, delay=1)
    async def _process_chunk(self, chunk: str, client: httpx.AsyncClient, params: Dict) -> str:
        """Process a single chunk of text with retries"""
        try:
            start_time = datetime.utcnow()
            response = await client.post(
                "/generate",
                json={
                    "inputs": chunk,
                    "parameters": {
                        "max_length": params.get('max_length', self.settings.max_length),
                        "min_length": params.get('min_length', self.settings.min_length),
                        "do_sample": params.get('do_sample', False),
                        "num_beams": params.get('num_beams', self.settings.num_beams),
                        "length_penalty": params.get('length_penalty', self.settings.length_penalty),
                        "model_name": params.get('model_name', self.settings.default_model)
                    }
                }
            )
            response.raise_for_status()
            logger.info(f"Chunk processed in {(datetime.utcnow() - start_time).total_seconds():.2f}s")
            return response.json()["summary"]
            
        except httpx.HTTPError as e:
            logger.error(f"HTTP error processing chunk: {str(e)}")
            raise HTTPException(status_code=503, detail="Summarization service unavailable")
            
        except Exception as e:
            logger.error(f"Error processing chunk: {str(e)}")
            raise HTTPException(status_code=500, detail=str(e))

    async def summarize_text(
        self, 
        request: SummarizationRequest,
        background_tasks: BackgroundTasks,
        max_length: Optional[int] = None
    ) -> SummaryResponse:
        """
        Summarize text with support for long documents through chunking
        
        Args:
            request: SummarizationRequest containing text and parameters
            background_tasks: FastAPI BackgroundTasks for cleanup
            max_length: Optional override for max_length parameter
            
        Returns:
            SummaryResponse with generated summary and metadata
        """
        start_time = datetime.utcnow()
        
        try:
            # Update request parameters with any overrides
            request_params = request.dict()
            if max_length:
                request_params['max_length'] = max_length

            async with await self._get_client() as client:
                # For short texts, process directly
                if len(request.text.split()) <= self.chunk_size:
                    summary = await self._process_chunk(request.text, client, request_params)
                
                # For long texts, process in chunks with controlled concurrency
                else:
                    chunks = self._chunk_text(request.text, self.chunk_size)
                    logger.info(f"Processing text in {len(chunks)} chunks")
                    
                    semaphore = asyncio.Semaphore(self.max_parallel_requests)
                    
                    async def process_with_semaphore(chunk: str) -> str:
                        async with semaphore:
                            return await self._process_chunk(chunk, client, request_params)
                    
                    chunk_summaries = await asyncio.gather(
                        *[process_with_semaphore(chunk) for chunk in chunks],
                        return_exceptions=True
                    )
                    
                    # Handle any failed chunks
                    failed_chunks = [i for i, x in enumerate(chunk_summaries) if isinstance(x, Exception)]
                    if failed_chunks:
                        logger.error(f"Failed to process chunks: {failed_chunks}")
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to process {len(failed_chunks)} chunks"
                        )
                    
                    # Combine chunk summaries and create final summary if needed
                    combined_summary = " ".join(chunk_summaries)
                    if len(combined_summary.split()) > request_params.get('max_length', self.settings.max_length):
                        summary = await self._process_chunk(combined_summary, client, request_params)
                    else:
                        summary = combined_summary

                processing_time = (datetime.utcnow() - start_time).total_seconds()
                
                # Create response with metadata
                response = SummaryResponse(
                    summary=summary,
                    model_used=request_params.get('model_name', self.settings.default_model),
                    processing_time=processing_time,
                    text_length=len(request.text.split()),
                    summary_length=len(summary.split()),
                    metadata={
                        "chunks_processed": len(chunks) if len(request.text.split()) > self.chunk_size else 1,
                        "parameters": request_params,
                        "transformer_service": self.transformer_url
                    }
                )
                
                # Add cleanup task
                background_tasks.add_task(self._cleanup_resources, request.text)
                
                return response

        except asyncio.TimeoutError:
            logger.error("Timeout while connecting to transformer service")
            raise HTTPException(status_code=504, detail="Summarization service timeout")
            
        except Exception as e:
            logger.error(f"Error in summarization service: {str(e)}", exc_info=True)
            raise HTTPException(status_code=500, detail=str(e))

    async def batch_summarize(
        self,
        requests: List[SummarizationRequest],
        background_tasks: BackgroundTasks
    ) -> List[SummaryResponse]:
        """Process multiple summarization requests in parallel"""
        tasks = [
            self.summarize_text(req, background_tasks)
            for req in requests
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)

    async def health_check(self) -> Dict:
        """Check if the transformer service is healthy"""
        try:
            async with await self._get_client() as client:
                start_time = datetime.utcnow()
                response = await client.get("/health")
                latency = (datetime.utcnow() - start_time).total_seconds() * 1000
                
                if response.status_code == 200:
                    return {
                        "healthy": True,
                        "details": {
                            "transformer_service": "healthy",
                            "latency_ms": latency,
                            "model_loaded": True
                        }
                    }
                return {
                    "healthy": False,
                    "details": {
                        "transformer_service": "unhealthy",
                        "status_code": response.status_code
                    }
                }
        except Exception as e:
            logger.error(f"Health check failed: {str(e)}")
            return {
                "healthy": False,
                "details": {
                    "transformer_service": "unhealthy",
                    "error": str(e)
                }
            }

    async def _cleanup_resources(self, text: str):
        """Cleanup any temporary resources"""
        try:
            # Implement resource cleanup as needed
            pass
        except Exception as e:
            logger.error(f"Cleanup failed: {str(e)}")

# Don't create a global instance - use dependency injection instead

--------------------------------


"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\routes\health.py"


# services/backend-fastapi/api/routes/health.py

from fastapi import APIRouter
from typing import Dict
from datetime import datetime

router = APIRouter(tags=["health"])

@router.get("/health")
async def health_check():
    """Enhanced health check including parallel processing components"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow(),
        "components": {
            "pdf_processor": "healthy",
            "ocr_worker": "healthy",
            "neural_merger": "healthy",
            "parallel_processing": "enabled",
            "quality_scoring": "enabled"
        },
        "version": "2.0.0 TITAN"
    }

@router.get("/health/detailed")
async def detailed_health():
    """Detailed health status of all processing components"""
    return {
        "status": "healthy",
        "components": {
            "text_extraction": {
                "status": "healthy",
                "mode": "parallel",
                "workers": "active"
            },
            "ocr_processing": {
                "status": "healthy",
                "mode": "parallel",
                "service": "connected"
            },
            "neural_merger": {
                "status": "healthy",
                "model": "active"
            },
            "quality_scoring": {
                "status": "healthy",
                "metrics": "enabled"
            }
        },
        "system": {
            "memory_usage": "nominal",
            "processing_queue": "ready",
            "storage": "available"
        }
    }

-----------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\routes\ocr_routes.py"
# backend-fastapi/api/routes/ocr_routes.py
from fastapi import APIRouter, UploadFile, File, Depends
from ..dependencies import get_ocr_service
from ..ocr_service import OCRService
from typing import Dict, Any

router = APIRouter(prefix="/api/v1/ocr", tags=["ocr"])

@router.get("/health")
async def check_ocr_health(
    ocr_service: OCRService = Depends(get_ocr_service)
) -> Dict[str, Any]:
    return await ocr_service.health_check()

@router.post("/extract")
async def extract_text(
    file: UploadFile = File(...),
    ocr_service: OCRService = Depends(get_ocr_service)
) -> Dict[str, Any]:
    return await ocr_service.extract_text(file)


------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\routes\pdf_routes.py"

# services/backend-fastapi/api/routes/pdf_routes.py

from fastapi import APIRouter, File, UploadFile, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from typing import Dict, Optional
import uuid
from datetime import datetime
import aiofiles
from loguru import logger

from ..pdf_pipeline import ProcessingOrchestrator
from ...models import ProcessingStatus, ProcessingResult
from ...config.settings import Settings

router = APIRouter(prefix="/pdf", tags=["pdf"])
settings = Settings()

# Initialize our enhanced orchestrator
orchestrator = ProcessingOrchestrator()

# Store processing status
processing_tasks: Dict[str, ProcessingStatus] = {}

@router.post("/process")
async def process_pdf(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    processing_options: Optional[Dict] = None
):
    """
    Process a PDF using parallel architecture
    """
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")
    
    task_id = str(uuid.uuid4())
    file_path = orchestrator.upload_dir / f"{task_id}.pdf"
    
    try:
        content = await file.read()
        if len(content) > settings.max_file_size:
            raise HTTPException(
                status_code=400,
                detail=f"File too large. Maximum size: {settings.max_file_size // (1024*1024)}MB"
            )
        
        # Save uploaded file
        async with aiofiles.open(file_path, 'wb') as f:
            await f.write(content)
        
        # Initialize status
        processing_tasks[task_id] = ProcessingStatus(
            task_id=task_id,
            filename=file.filename,
            status="QUEUED",
            timestamp=datetime.utcnow()
        )
        
        # Start parallel processing
        background_tasks.add_task(
            orchestrator.process_pdf,
            task_id,
            file_path
        )
        
        return JSONResponse({
            "task_id": task_id,
            "status": "QUEUED",
            "message": "PDF-DESTROYER-MAX TITAN processing initiated"
        })
        
    except Exception as e:
        logger.error(f"Error initiating PDF processing: {str(e)}")
        if file_path.exists():
            file_path.unlink()
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/status/{task_id}")
async def get_status(task_id: str):
    """Get processing status"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    return processing_tasks[task_id]

@router.get("/result/{task_id}")
async def get_result(task_id: str):
    """Get processing results"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    
    status = processing_tasks[task_id]
    if status.status != "COMPLETED":
        raise HTTPException(
            status_code=400,
            detail=f"Task is not completed. Current status: {status.status}"
        )
    
    return status.result



----------------------------------


"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\api\routes\summarization_routes.py"


from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends, Query
from ..summarization_service import SummarizationService, SummarizationRequest, SummaryResponse
from ..dependencies import get_summarization_service
from typing import List, Optional
import asyncio
import logging
from pydantic import constr

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/api/summarize", tags=["summarization"])

@router.post(
    "/",
    response_model=SummaryResponse,
    summary="Summarize text",
    description="Generate a summary of the provided text using transformers",
    response_description="Generated summary with metadata"
)
async def summarize_text(
    request: SummarizationRequest,
    background_tasks: BackgroundTasks,
    max_length: Optional[int] = Query(300, gt=0, le=1000),
    summarization_service: SummarizationService = Depends(get_summarization_service)
):
    """
    Generate a summary from input text with configurable parameters
    
    - **text**: Input text to summarize
    - **max_length**: Maximum length of generated summary
    - **min_length**: Minimum length of generated summary
    - **model_name**: Name of model to use (optional)
    """
    try:
        request.validate_text()
        return await summarization_service.summarize_text(
            request, 
            background_tasks,
            max_length=max_length
        )
    except ValueError as ve:
        raise HTTPException(status_code=400, detail=str(ve))
    except Exception as e:
        logger.error(f"Error in summarize endpoint: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Summarization failed")

@router.post(
    "/batch",
    response_model=List[SummaryResponse],
    summary="Batch summarize texts",
    description="Generate summaries for multiple texts in parallel"
)
async def batch_summarize(
    requests: List[SummarizationRequest],
    background_tasks: BackgroundTasks,
    max_batch_size: int = Query(10, gt=0, le=50),
    summarization_service: SummarizationService = Depends(get_summarization_service)
):
    """Process multiple texts in parallel with controlled concurrency"""
    if len(requests) > max_batch_size:
        raise HTTPException(
            status_code=400,
            detail=f"Batch size exceeds maximum allowed ({max_batch_size})"
        )
        
    try:
        tasks = [
            summarization_service.summarize_text(req, background_tasks)
            for req in requests
        ]
        return await asyncio.gather(*tasks)
    except Exception as e:
        logger.error(f"Error in batch summarize endpoint: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Batch summarization failed")

@router.get(
    "/health",
    summary="Check summarization service health",
    description="Verify the summarization service and model are functioning"
)
async def health_check(
    summarization_service: SummarizationService = Depends(get_summarization_service)
):
    """Check health of summarization service and model"""
    try:
        status = await summarization_service.health_check()
        if status["healthy"]:
            return status
        raise HTTPException(
            status_code=503, 
            detail=f"Summarization service unhealthy: {status.get('details')}"
        )
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=503,
            detail="Health check failed"
        )
---------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\config\settings.py"

from pydantic_settings import BaseSettings
from typing import List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # API Settings
    api_version: str = "1.0.0"
    debug_mode: bool = os.getenv("DEBUG_MODE", True)
    host: str = os.getenv("HOST", "0.0.0.0")
    port: int = int(os.getenv("PORT", 8001))
    worker_count: int = int(os.getenv("WORKER_COUNT", 1))

    # CORS Settings
    cors_origins: List[str] = ["*"]  # In production, replace with actual origins

    # Logging
    log_level: str = os.getenv("LOG_LEVEL", "INFO")

    # Transformer Service Settings
    transformer_service_url: str = os.getenv(
        "TRANSFORMER_SERVICE_URL", 
        "http://transformers-summarizer:8006"
    )
    transformer_timeout: int = int(os.getenv("TRANSFORMER_TIMEOUT", 30))
    max_retries: int = int(os.getenv("MAX_RETRIES", 3))
    batch_size: int = int(os.getenv("BATCH_SIZE", 10))

    # Model Settings
    default_model: str = os.getenv("DEFAULT_MODEL", "facebook/bart-large-cnn")
    max_length: int = int(os.getenv("MAX_LENGTH", 300))
    min_length: int = int(os.getenv("MIN_LENGTH", 100))
    length_penalty: float = float(os.getenv("LENGTH_PENALTY", 2.0))
    num_beams: int = int(os.getenv("NUM_BEAMS", 4))

    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    return Settings()


-----------------------------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\domain\models.py"


from pydantic_settings import BaseSettings
from typing import List
import os
from functools import lru_cache

class Settings(BaseSettings):
    # API Settings
    api_version: str = "1.0.0"
    debug_mode: bool = os.getenv("DEBUG_MODE", True)
    host: str = os.getenv("HOST", "0.0.0.0")
    port: int = int(os.getenv("PORT", 8001))
    worker_count: int = int(os.getenv("WORKER_COUNT", 1))

    # CORS Settings
    cors_origins: List[str] = ["*"]  # In production, replace with actual origins

    # Logging
    log_level: str = os.getenv("LOG_LEVEL", "INFO")

    # Transformer Service Settings
    transformer_service_url: str = os.getenv(
        "TRANSFORMER_SERVICE_URL", 
        "http://transformers-summarizer:8006"
    )
    transformer_timeout: int = int(os.getenv("TRANSFORMER_TIMEOUT", 30))
    max_retries: int = int(os.getenv("MAX_RETRIES", 3))
    batch_size: int = int(os.getenv("BATCH_SIZE", 10))

    # Model Settings
    default_model: str = os.getenv("DEFAULT_MODEL", "facebook/bart-large-cnn")
    max_length: int = int(os.getenv("MAX_LENGTH", 300))
    min_length: int = int(os.getenv("MIN_LENGTH", 100))
    length_penalty: float = float(os.getenv("LENGTH_PENALTY", 2.0))
    num_beams: int = int(os.getenv("NUM_BEAMS", 4))

    class Config:
        env_file = ".env"
        case_sensitive = False

@lru_cache()
def get_settings() -> Settings:
    return Settings()






-----------------------------------------------

# services/backend-fastapi/domain/types.py


# services/backend-fastapi/domain/types.py

from enum import Enum
from typing import Dict, List, Optional, Union
from pydantic import BaseModel, Field
from datetime import datetime

class ProcessingStatus(str, Enum):
    """Status of document processing"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class ProcessorType(str, Enum):
    """Types of document processors"""
    TEXT_EXTRACTION = "text_extraction"
    OCR = "ocr"
    LAYOUT_ANALYSIS = "layout_analysis"
    TABLE_DETECTION = "table_detection"
    SUMMARIZATION = "summarization"

class DocumentType(str, Enum):
    """Supported document types"""
    PDF = "pdf"
    IMAGE = "image"
    SCANNED_PDF = "scanned_pdf"
    DIGITAL_PDF = "digital_pdf"

class ExportFormat(str, Enum):
    """Supported export formats"""
    PDF = "pdf"
    TEXT = "text"
    JSON = "json"
    HTML = "html"
    MARKDOWN = "markdown"
    CSV = "csv"

class ProcessingOptions(BaseModel):
    """Options for document processing"""
    processors: List[ProcessorType] = Field(
        default=[ProcessorType.TEXT_EXTRACTION],
        description="List of processors to run"
    )
    parallel_processing: bool = Field(
        default=True,
        description="Whether to run processors in parallel"
    )
    quality_threshold: float = Field(
        default=0.7,
        description="Minimum quality score threshold",
        ge=0.0,
        le=1.0
    )
    export_format: ExportFormat = Field(
        default=ExportFormat.PDF,
        description="Desired export format"
    )
    preserve_layout: bool = Field(
        default=True,
        description="Whether to preserve document layout"
    )

class ProcessingResult(BaseModel):
    """Result from a single processor"""
    processor_type: ProcessorType
    confidence_score: float = Field(ge=0.0, le=1.0)
    text_content: str
    metadata: Dict = Field(default_factory=dict)
    page_number: int
    bounding_boxes: Optional[List[Dict[str, float]]] = None

class DocumentMetadata(BaseModel):
    """Metadata for a document"""
    document_id: str
    filename: str
    file_size: int
    page_count: int
    document_type: DocumentType
    upload_time: datetime
    processing_status: ProcessingStatus
    processing_options: ProcessingOptions
    last_modified: datetime
    checksum: str

class ProcessingSummary(BaseModel):
    """Summary of document processing"""
    document_id: str
    status: ProcessingStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    error_message: Optional[str] = None
    processor_statuses: Dict[ProcessorType, ProcessingStatus]
    quality_scores: Dict[ProcessorType, float]
    processing_time: Optional[float] = None

class ErrorResponse(BaseModel):
    """Standardized error response"""
    error_code: str
    message: str
    details: Optional[Dict] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class HealthStatus(BaseModel):
    """Service health status"""
    status: str
    version: str
    uptime: float
    processor_status: Dict[ProcessorType, bool]
    resource_usage: Dict[str, float]
    last_check: datetime = Field(default_factory=datetime.utcnow)




-----------------------------------------------


"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\tests\test_backend_fastapi.py"

# services/backend-fastapi/tests/test_backend_fastapi.py

import pytest
from fastapi.testclient import TestClient
from main import app
import asyncio
from unittest.mock import Mock, patch
import os
import json
from datetime import datetime

client = TestClient(app)

@pytest.fixture
def mock_ocr_service():
    with patch("api.dependencies.OCRService") as mock:
        yield mock

@pytest.fixture
def mock_summarization_service():
    with patch("api.dependencies.SummarizationService") as mock:
        yield mock

@pytest.fixture
def mock_pdf_pipeline():
    with patch("api.dependencies.PDFPipelineService") as mock:
        yield mock

def test_root():
    """Test root endpoint"""
    response = client.get("/")
    assert response.status_code == 200
    assert "PDF Destroyer Max API is running" in response.json()["message"]

def test_health_check():
    """Test health check endpoint"""
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

@pytest.mark.asyncio
async def test_pdf_processing(mock_pdf_pipeline):
    """Test PDF processing endpoint"""
    # Create a test PDF file
    test_pdf_content = b"%PDF-1.4 test content"
    with open("test.pdf", "wb") as f:
        f.write(test_pdf_content)

    try:
        with open("test.pdf", "rb") as f:
            response = client.post(
                "/pdf/process",
                files={"file": ("test.pdf", f, "application/pdf")}
            )
        
        assert response.status_code == 200
        result = response.json()
        assert "task_id" in result
        assert result["status"] == "QUEUED"

    finally:
        # Cleanup
        if os.path.exists("test.pdf"):
            os.remove("test.pdf")

@pytest.mark.asyncio
async def test_summarization(mock_summarization_service):
    """Test text summarization endpoint"""
    test_text = "This is a test text that needs to be summarized."
    
    # Mock the summarization service response
    mock_summary = {
        "summary": "Test summary",
        "model_used": "test-model",
        "processing_time": 0.5,
        "text_length": len(test_text.split()),
        "summary_length": 2,
        "timestamp": datetime.utcnow().isoformat(),
        "metadata": {}
    }
    
    mock_summarization_service.return_value.summarize_text.return_value = mock_summary

    response = client.post(
        "/api/summarize/",
        json={
            "text": test_text,
            "max_length": 50,
            "min_length": 10
        }
    )
    
    assert response.status_code == 200
    result = response.json()
    assert "summary" in result
    assert result["summary"] == "Test summary"

@pytest.mark.asyncio
async def test_ocr_processing(mock_ocr_service):
    """Test OCR processing endpoint"""
    # Create a test image
    test_image_content = b"fake image content"
    with open("test_image.png", "wb") as f:
        f.write(test_image_content)

    try:
        # Mock OCR service response
        mock_ocr_service.return_value.extract_text.return_value = {
            "text

-----------------------------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\utils\helpers.py"

from fastapi import APIRouter, HTTPException, Depends
from ..summarization_service import get_summarization_service, SummarizationService
from ...domain.models import SummarizationRequest, SummarizationResponse

router = APIRouter()

@router.post("/summarize", response_model=SummarizationResponse)
async def summarize_text(
    request: SummarizationRequest,
    summarization_service: SummarizationService = Depends(get_summarization_service)
):
    try:
        result = await summarization_service.summarize_text(
            text=request.text,
            max_length=request.max_length,
            min_length=request.min_length,
            do_sample=request.do_sample
        )
        return SummarizationResponse(summary=result["summary"])
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/batch-summarize")
async def batch_summarize(
    texts: list[str],
    summarization_service: SummarizationService = Depends(get_summarization_service)
):
    try:
        results = await summarization_service.batch_summarize(
            texts=texts,
            max_concurrent=3
        )
        return {"summaries": [r["summary"] for r in results if isinstance(r, dict)]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


-----------------------------------

"C:\Users\JJ\deep-ai\pdf-destroyer-max\services\backend-fastapi\utils\logging.py"

# services/backend-fastapi/utils/logging.py

from loguru import logger
import sys
from pathlib import Path
import json
from datetime import datetime
import logging
import functools

# Configure base logging path
LOG_DIR = Path(__file__).parent.parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

# Remove default logger and add our custom configurations
logger.remove()

# Console logging with color
logger.add(
    sys.stdout,
    colorize=True,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)

# File logging with JSON format
logger.add(
    LOG_DIR / "app.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    rotation="500 MB",
    retention="10 days",
    compression="zip"
)

# JSON logging for structured data
logger.add(
    LOG_DIR / "structured.json",
    serialize=True,  # Enable JSON serialization
    format="{time} | {level} | {message}",
    rotation="100 MB"
)

def log_error(func):
    """Decorator to log function errors with full traceback"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            logger.exception(
                f"Error in {func.__name__}",
                function=func.__name__,
                args=args,
                kwargs=kwargs,
                error=str(e),
                error_type=type(e).__name__
            )
            raise
    return wrapper

def log_request(func):
    """Decorator to log API requests"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = datetime.utcnow()
        try:
            result = await func(*args, **kwargs)
            duration = (datetime.utcnow() - start_time).total_seconds()
            
            logger.info(
                f"Request processed",
                function=func.__name__,
                duration_seconds=duration,
                success=True
            )
            return result
        except Exception as e:
            duration = (datetime.utcnow() - start_time).total_seconds()
            logger.error(
                f"Request failed",
                function=func.__name__,
                duration_seconds=duration,
                success=False,
                error=str(e)
            )
            raise
    return wrapper

# Intercept standard library logging
class InterceptHandler(logging.Handler):
    def emit(self, record):
        # Get corresponding Loguru level if it exists
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message
        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(
            level, record.getMessage()
        )

# Setup logging configuration
def setup_logging():
    """Configure logging to use loguru for all logs"""
    # Intercept standard library logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)
    
    # List of loggers to intercept
    logging_modules = [
        'uvicorn',
        'uvicorn.error',
        'fastapi',
        'gunicorn',
        'aiohttp',
        'multiprocessing'
    ]
    
    for module in logging_modules:
        logging_logger = logging.getLogger(module)
        logging_logger.handlers = [InterceptHandler()]

# Example usage in routes:
"""
from utils.logging import logger, log_request, log_error

@router.post("/process")
@log_request
@log_error
async def process_pdf(file: UploadFile):
    logger.info(f"Processing PDF: {file.filename}")
    # ... rest of the code


"""




# backend-fastapi/__init__.py
"""
PDF Destroyer Max Backend FastAPI Service
"""
from .api import routes
from .config import settings
from .utils import logging

__version__ = "1.0.0"

# api/__init__.py
"""
API module containing routes and services
"""
from .routes import health, pdf_routes, summarization_routes, ocr_routes
from .summarization_service import SummarizationService
from .ocr_service import OCRService
from .pdf_pipeline import ProcessingOrchestrator

__all__ = [
    'health',
    'pdf_routes',
    'summarization_routes',
    'ocr_routes',
    'SummarizationService',
    'OCRService',
    'ProcessingOrchestrator'
]

# api/routes/__init__.py
"""
API routes module
"""
from .health import router as health_router
from .pdf_routes import router as pdf_router
from .summarization_routes import router as summarization_router
from .ocr_routes import router as ocr_router

__all__ = [
    'health_router',
    'pdf_router',
    'summarization_router',
    'ocr_router'
]

# config/__init__.py
"""
Configuration module
"""
from .settings import Settings, get_settings

__all__ = ['Settings', 'get_settings']

# domain/__init__.py
"""
Domain models and types
"""
from .models import *
from .types import (
    ProcessingStatus,
    ProcessorType,
    DocumentType,
    ExportFormat,
    ProcessingOptions,
    ProcessingResult,
    DocumentMetadata,
    ProcessingSummary,
    ErrorResponse,
    HealthStatus
)

__all__ = [
    'ProcessingStatus',
    'ProcessorType',
    'DocumentType',
    'ExportFormat',
    'ProcessingOptions',
    'ProcessingResult',
    'DocumentMetadata',
    'ProcessingSummary',
    'ErrorResponse',
    'HealthStatus'
]

# utils/__init__.py
"""
Utility functions and helpers
"""
from .logging import logger, setup_logging, log_error, log_request
from .helpers import async_retry

__all__ = [
    'logger',
    'setup_logging',
    'log_error',
    'log_request',
    'async_retry'
]

# tests/__init__.py
"""
Test suite for the FastAPI backend
"""

# ai_powerhouse_pipelines/__init__.py
"""
AI Powerhouse Pipelines - Core ML processing pipeline
"""
from .src.orchestration.smart_orchestrator import SmartOrchestrator
from .src.models.quality_scoring.scorer import QualityScorer
from .src.models.neural_merger.merger import NeuralMerger

__version__ = "1.0.0"
__all__ = ['SmartOrchestrator', 'QualityScorer', 'NeuralMerger']

# ai_powerhouse_pipelines/src/__init__.py
"""
Source package containing core implementation
"""
from . import models
from . import orchestration

# ai_powerhouse_pipelines/src/models/__init__.py
"""
ML models module
"""
from .quality_scoring.scorer import QualityScorer
from .neural_merger.merger import NeuralMerger

__all__ = ['QualityScorer', 'NeuralMerger']

# ai_powerhouse_pipelines/src/models/neural_merger/__init__.py
"""
Neural merger module for combining multiple processing results
"""
from .merger import NeuralMerger

__all__ = ['NeuralMerger']

# ai_powerhouse_pipelines/src/models/quality_scoring/__init__.py
"""
Quality scoring module for assessing processing quality
"""
from .scorer import QualityScorer

__all__ = ['QualityScorer']

# ai_powerhouse_pipelines/src/orchestration/__init__.py
"""
Orchestration module for coordinating processing pipeline
"""
from .smart_orchestrator import SmartOrchestrator

__all__ = ['SmartOrchestrator']